#!/usr/bin/env python3
"""
Scraper com ScraperAPI - Jarbas Imobili√°rio
Usa ScraperAPI como alternativa √† Apify
"""

import requests
import json
import time
import re
from datetime import datetime
from urllib.parse import quote

# Configura√ß√£o ScraperAPI
SCRAPERAPI_KEY = os.getenv('SCRAPERAPI_KEY', 'seu_token_aqui')
SCRAPERAPI_URL = "http://api.scraperapi.com"

# Sites de leil√µes
SITES = [
    {"nome": "Leilosoc", "url": "https://www.leilosoc.com/pt/leiloes/?categoria=imoveis", "ativo": True},
    {"nome": "Venda Judicial", "url": "https://www.vendajudicial.pt", "ativo": True},
    {"nome": "Aval Ib√©rica", "url": "https://www.avaliberica.pt/leiloes/", "ativo": True},
    {"nome": "LC Premium", "url": "https://www.lcpremium.pt", "ativo": True},
    {"nome": "Exclusiva Agora", "url": "https://www.exclusivagora.com", "ativo": True},
    {"nome": "Capital Leiloeira", "url": "https://www.capital-leiloeira.pt", "ativo": True},
]

def scrape_with_scraperapi(url, nome):
    """Faz scraping usando ScraperAPI"""
    print(f"\nüìå {nome}")
    print(f"   URL: {url}")
    
    # Construir URL da ScraperAPI
    api_url = f"{SCRAPERAPI_URL}/?api_key={SCRAPERAPI_KEY}&url={quote(url)}&render=true&country_code=pt"
    
    try:
        response = requests.get(api_url, timeout=60)
        
        if response.status_code == 200:
            html = response.text
            print(f"   ‚úÖ HTML recebido: {len(html)} bytes")
            
            # Extrair im√≥veis do HTML
            imoveis = extrair_imoveis_do_html(html, nome, url)
            print(f"   üè† Im√≥veis encontrados: {len(imoveis)}")
            
            return {
                "site": nome,
                "url": url,
                "status": response.status_code,
                "html_size": len(html),
                "imoveis": imoveis,
                "timestamp": datetime.now().isoformat()
            }
        else:
            print(f"   ‚ùå Erro HTTP {response.status_code}")
            return None
            
    except Exception as e:
        print(f"   ‚ùå Erro: {str(e)[:80]}")
        return None

def extrair_imoveis_do_html(html, fonte, base_url):
    """Extrai dados de im√≥veis do HTML"""
    imoveis = []
    
    # Padr√µes comuns para im√≥veis em sites de leil√£o
    padroes = [
        # Padr√£o 1: Cards com pre√ßo e localiza√ß√£o
        r'([\w\s\-]+(?:T[0-5]|Moradia|Apartamento|Loja|Terreno)[\w\s\-]*).*?(\d+[\s\.]?\d+)\s*‚Ç¨.*?([\w\s]+(?:Lisboa|Porto|Cascais|Sintra|Oeiras|Loures|Amadora|Almada|Seixal|Set√∫bal)[\w\s]*)',
        # Padr√£o 2: Pre√ßo seguido de √°rea
        r'(\d+[\s\.]?\d+)\s*‚Ç¨.*?([\d\.]+)\s*m[¬≤2].*?(T[0-5]|Moradia)',
        # Padr√£o 3: T√≠tulos de im√≥veis
        r'(Leil√£o.*?T[0-5].*?)(\d+[\s\.]?\d+)\s*‚Ç¨',
    ]
    
    # Procurar por padr√µes no HTML
    for padrao in padroes:
        matches = re.findall(padrao, html, re.IGNORECASE | re.DOTALL)
        for match in matches[:10]:  # Limitar a 10 por padr√£o
            if isinstance(match, tuple):
                texto = ' '.join(str(m) for m in match if m)
            else:
                texto = str(match)
            
            # Extrair pre√ßo
            preco_match = re.search(r'(\d+[\s\.]?\d+)\s*‚Ç¨', texto)
            preco = None
            if preco_match:
                preco_str = preco_match.group(1).replace('.', '').replace(' ', '')
                preco = float(preco_str) if preco_str.isdigit() else None
            
            # Extrair √°rea
            area_match = re.search(r'(\d+)\s*m[¬≤2]', texto, re.IGNORECASE)
            area = int(area_match.group(1)) if area_match else None
            
            # Extrair tipologia
            tipos = ['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'Moradia', 'Loja', 'Terreno']
            tipologia = None
            for tipo in tipos:
                if tipo.lower() in texto.lower():
                    tipologia = tipo
                    break
            
            if preco and preco > 10000:  # Filtrar pre√ßos v√°lidos
                imovel = {
                    "id": f"{fonte.lower().replace(' ', '_')}_{len(imoveis)}",
                    "titulo": texto[:150].strip(),
                    "preco": preco,
                    "preco_texto": f"‚Ç¨{preco:,.0f}",
                    "area": area,
                    "tipologia": tipologia or "Im√≥vel",
                    "fonte": fonte,
                    "url": base_url,
                    "data_extracao": datetime.now().isoformat()
                }
                imoveis.append(imovel)
    
    # Remover duplicados baseado no t√≠tulo
    imoveis_unicos = []
    titulos_vistos = set()
    for imo in imoveis:
        titulo_base = imo["titulo"][:50].lower()
        if titulo_base not in titulos_vistos:
            titulos_vistos.add(titulo_base)
            imoveis_unicos.append(imo)
    
    return imoveis_unicos

def main():
    print("=" * 70)
    print("üè† SCRAPERAPI - EXTRATOR DE IM√ìVEIS")
    print("=" * 70)
    print(f"‚è∞ In√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üîë API Key: {SCRAPERAPI_KEY[:10]}...")
    
    resultados = []
    todos_imoveis = []
    
    for site in SITES:
        if not site["ativo"]:
            continue
            
        resultado = scrape_with_scraperapi(site["url"], site["nome"])
        if resultado:
            resultados.append(resultado)
            todos_imoveis.extend(resultado.get("imoveis", []))
        
        time.sleep(2)  # Respeitar rate limits
    
    # Guardar resultados detalhados
    output = {
        "data_extracao": datetime.now().isoformat(),
        "total_sites": len(SITES),
        "sites_sucesso": len(resultados),
        "total_imoveis": len(todos_imoveis),
        "resultados": resultados
    }
    
    with open("extracao_scraperapi_detalhes.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    # Guardar apenas os im√≥veis (formato simplificado)
    imoveis_output = {
        "data_extracao": datetime.now().isoformat(),
        "total_imoveis": len(todos_imoveis),
        "imoveis": todos_imoveis
    }
    
    with open("imoveis_scraperapi.json", "w", encoding="utf-8") as f:
        json.dump(imoveis_output, f, ensure_ascii=False, indent=2)
    
    # Resumo
    print("\n" + "=" * 70)
    print("üìä RESUMO DA EXTRA√á√ÉO")
    print("=" * 70)
    print(f"Sites testados: {len(SITES)}")
    print(f"Sites com resposta: {len(resultados)}")
    print(f"Total de im√≥veis extra√≠dos: {len(todos_imoveis)}")
    
    # Estat√≠sticas por site
    print("\nüìà Por site:")
    for r in resultados:
        count = len(r.get("imoveis", []))
        print(f"   ‚Ä¢ {r['site']}: {count} im√≥veis")
    
    # Amostra de im√≥veis
    if todos_imoveis:
        print("\nüè† Amostra de im√≥veis encontrados:")
        for i, imo in enumerate(todos_imoveis[:5], 1):
            print(f"\n   {i}. [{imo['fonte']}] {imo['tipologia']}")
            print(f"      Pre√ßo: {imo['preco_texto']}")
            if imo.get('area'):
                print(f"      √Årea: {imo['area']} m¬≤")
            print(f"      {imo['titulo'][:80]}...")
    
    print(f"\nüíæ Ficheiros guardados:")
    print(f"   ‚Ä¢ extracao_scraperapi_detalhes.json (detalhes completos)")
    print(f"   ‚Ä¢ imoveis_scraperapi.json (apenas im√≥veis)")
    
    return todos_imoveis

if __name__ == "__main__":
    imoveis = main()
