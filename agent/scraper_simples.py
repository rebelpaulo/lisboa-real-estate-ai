#!/usr/bin/env python3
"""
Scraper Simples - Teste rÃ¡pido com output imediato
"""

import asyncio
import json
import re
from datetime import datetime
from playwright.async_api import async_playwright

SITES = [
    {"nome": "leilosoc.com", "url": "https://www.leilosoc.com/pt/leiloes/?categoria=imoveis"},
    {"nome": "vendajudicial.pt", "url": "https://vendajudicial.pt/"},
    {"nome": "avaliberica.pt", "url": "https://www.avaliberica.pt/leiloes/"},
    {"nome": "lcpremium.pt", "url": "https://www.lcpremium.pt/"},
    {"nome": "exclusivagora.com", "url": "https://www.exclusivagora.com/"},
    {"nome": "capital-leiloeira.pt", "url": "https://www.capital-leiloeira.pt/"},
]

print("=" * 70)
print("ðŸ  SCRAPER SIMPLES - TESTE RÃPIDO")
print("=" * 70)
print(f"â° InÃ­cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ðŸ“ Sites a processar: {len(SITES)}")
print("")

async def scrape_site(browser, site):
    """Scraper simples para um site"""
    print(f"ðŸ“Œ {site['nome']} - A iniciar...", flush=True)
    
    imoveis = []
    
    try:
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        )
        page = await context.new_page()
        
        print(f"   ðŸŒ A navegar para {site['url'][:50]}...", flush=True)
        
        # Navegar com timeout
        response = await page.goto(site['url'], timeout=30000, wait_until='domcontentloaded')
        
        print(f"   âœ… PÃ¡gina carregada (Status: {response.status if response else 'N/A'})", flush=True)
        
        # Aguardar um pouco para JS carregar
        await asyncio.sleep(3)
        
        # Extrair todo o texto da pÃ¡gina
        content = await page.content()
        text = await page.inner_text('body')
        
        print(f"   ðŸ“„ HTML: {len(content)} bytes | Texto: {len(text)} bytes", flush=True)
        
        # Procurar por padrÃµes de imÃ³veis
        # PadrÃ£o: T1, T2, Apartamento, Moradia + preÃ§o
        padroes_imoveis = [
            r'(T[0-5]|Apartamento|Moradia|Loja)[\s\w\-]+(?:\d+[\.\s]?\d+)\s*â‚¬',
            r'(\d+[\.\s]?\d+)\s*â‚¬[\s\w\-]+(?:T[0-5]|Apartamento|Moradia)',
        ]
        
        encontrados = 0
        for padrao in padroes_imoveis:
            matches = re.findall(padrao, text, re.IGNORECASE)
            encontrados += len(matches)
        
        print(f"   ðŸ” PadrÃµes de imÃ³veis encontrados: {encontrados}", flush=True)
        
        # Extrair links de imÃ³veis
        links = await page.query_selector_all('a')
        links_imoveis = []
        
        for link in links[:30]:  # Limitar a 30 links
            try:
                href = await link.get_attribute('href') or ""
                texto = await link.inner_text()
                
                # Filtrar links relevantes
                if any(kw in texto.lower() for kw in ['t1', 't2', 't3', 'apartamento', 'moradia', 'imovel', 'leilao', 'â‚¬']):
                    if len(texto) > 10 and len(texto) < 200:
                        links_imoveis.append({
                            'texto': texto.strip()[:100],
                            'url': href[:100] if href else ''
                        })
            except:
                pass
        
        print(f"   ðŸ”— Links de imÃ³veis: {len(links_imoveis)}", flush=True)
        
        # Criar entradas de imÃ³veis dos links
        for i, link in enumerate(links_imoveis[:10]):  # MÃ¡ximo 10 por site
            # Extrair preÃ§o
            preco_match = re.search(r'(\d+[\.\s]?\d+)\s*â‚¬', link['texto'])
            preco = None
            if preco_match:
                try:
                    preco_str = preco_match.group(1).replace('.', '').replace(' ', '')
                    preco = float(preco_str)
                except:
                    pass
            
            # Extrair tipologia
            tipos = ['T0', 'T1', 'T2', 'T3', 'T4', 'T5', 'Apartamento', 'Moradia', 'Loja']
            tipologia = None
            for tipo in tipos:
                if tipo.lower() in link['texto'].lower():
                    tipologia = tipo
                    break
            
            imoveis.append({
                'id': f"{site['nome'].split('.')[0]}_{i}",
                'fonte': site['nome'],
                'titulo': link['texto'],
                'tipologia': tipologia or 'ImÃ³vel',
                'preco': preco,
                'url': link['url'] if link['url'].startswith('http') else f"https://{site['nome']}{link['url']}",
                'data_extracao': datetime.now().isoformat()
            })
        
        await context.close()
        
        print(f"   âœ… ConcluÃ­do: {len(imoveis)} imÃ³veis extraÃ­dos", flush=True)
        
    except Exception as e:
        print(f"   âŒ Erro: {str(e)[:80]}", flush=True)
    
    return imoveis

async def main():
    todos_imoveis = []
    
    async with async_playwright() as p:
        print("ðŸš€ A lanÃ§ar browser...", flush=True)
        browser = await p.chromium.launch(headless=True)
        print("âœ… Browser pronto!\n", flush=True)
        
        for site in SITES:
            imoveis = await scrape_site(browser, site)
            todos_imoveis.extend(imoveis)
            print("")
        
        await browser.close()
    
    # Guardar resultados
    resultado = {
        'data_extracao': datetime.now().isoformat(),
        'total_imoveis': len(todos_imoveis),
        'imoveis': todos_imoveis
    }
    
    with open('dados_scraper_simples.json', 'w', encoding='utf-8') as f:
        json.dump(resultado, f, ensure_ascii=False, indent=2)
    
    print("=" * 70)
    print("ðŸ“Š RESUMO")
    print("=" * 70)
    print(f"Total de imÃ³veis: {len(todos_imoveis)}")
    
    # Por fonte
    fontes = {}
    for imo in todos_imoveis:
        fontes[imo['fonte']] = fontes.get(imo['fonte'], 0) + 1
    
    print("\nðŸ“ Por fonte:")
    for fonte, count in sorted(fontes.items(), key=lambda x: -x[1]):
        print(f"   â€¢ {fonte}: {count}")
    
    # Amostra
    if todos_imoveis:
        print("\nðŸ  Amostra:")
        for imo in todos_imoveis[:5]:
            preco_str = f"â‚¬{imo['preco']:,.0f}" if imo['preco'] else "PreÃ§o nÃ£o disponÃ­vel"
            print(f"   â€¢ [{imo['fonte']}] {imo['tipologia']} - {preco_str}")
            print(f"     {imo['titulo'][:60]}...")
    
    print(f"\nðŸ’¾ Guardado em: dados_scraper_simples.json")

if __name__ == "__main__":
    asyncio.run(main())
